{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3255412c",
   "metadata": {},
   "source": [
    "# Prompt Analysis\n",
    "Using NLP to determine which prompts our network got wrong to better understand which prompts are best for each diffusion model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3210eb",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "934002a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cpondoc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/cpondoc/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# For using NLTK later\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe9495",
   "metadata": {},
   "source": [
    "## Import Models\n",
    "For this case, we only import ResNet-18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "119c5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transferlearning import load_pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d078795",
   "metadata": {},
   "source": [
    "## Import Dataset Class and Transforms\n",
    "Used to help load in the images for heatmap generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b1cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.dataset import ImageDataset\n",
    "from utilities.transforms import data_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9869251",
   "metadata": {},
   "source": [
    "## Function to Determine Confusion Matrix\n",
    "The same framework as testing through the model, and then looking at prediction and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "880d2e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(transform, weights_path, batch_size, network, first, second):\n",
    "    # Loading in initial data\n",
    "    print(\"\\nLoading in data...\")\n",
    "    test_data = ImageDataset(\"test\", transform, 0.6, first, second)\n",
    "    testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "    print(\"Done loading in data.\")\n",
    "\n",
    "    # Test Loading\n",
    "    dataiter = iter(testloader)\n",
    "    images, labels, img_names = next(dataiter)\n",
    "\n",
    "    # Loading in a new example of the neural net, and loading in the weights\n",
    "    net = network\n",
    "    if (torch.cuda.is_available()):\n",
    "        net.to('cuda')\n",
    "        net.load_state_dict(torch.load(weights_path))\n",
    "    else:\n",
    "        net.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu')))\n",
    "    \n",
    "    # Generate confusion matrix -- also count all real but fake images, and all images\n",
    "    print(\"Generate Confusion Matrix\")\n",
    "    matrix = [[0, 0], [0, 0]]\n",
    "    real_but_fake = []\n",
    "    fake_and_fake = []\n",
    "    all_images = []\n",
    "    \n",
    "    # Perform same as testing/inferencing, but logging data.\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels, paths = data\n",
    "            for i in range(len(paths)):\n",
    "                all_images.append(paths[i])\n",
    "                \n",
    "            # Case for having a GPU available\n",
    "            images_cuda, labels_cuda = images, labels\n",
    "            if (torch.cuda.is_available()):\n",
    "                images_cuda, labels_cuda = images.cuda(), labels.cuda()\n",
    "            \n",
    "            # Feed the images through our network and evaluate them\n",
    "            outputs = net(images_cuda)\n",
    "            _, predicted = torch.max(outputs.cpu().data, 1)\n",
    "            \n",
    "            # Calculate the right part of the matrix of prediction and actual\n",
    "            for i in range(len(predicted)):\n",
    "                prediction = int(predicted[i].item())\n",
    "                actual = int(labels[i].item())\n",
    "                matrix[prediction][actual] += 1\n",
    "                \n",
    "                # Adding to set of images to further analyze if real, but fake\n",
    "                if (prediction is 1 and actual is 0):\n",
    "                    real_but_fake.append(paths[i])\n",
    "                elif (prediction is 0 and actual is 0):\n",
    "                    fake_and_fake.append(paths[i])\n",
    "    \n",
    "    return matrix, real_but_fake, fake_and_fake, all_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8650ec9",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "Runs all of the necessary functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73838e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_type, weights, first, second):\n",
    "    # Generate the necessary heatmaps\n",
    "    model = load_pretrained_model()\n",
    "    matrix, real_but_fake, fake_and_fake, all_images = generate_confusion_matrix(data_transforms[model_type], weights, 200, model, first, second)\n",
    "    \n",
    "    return matrix, real_but_fake, fake_and_fake, all_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f883657",
   "metadata": {},
   "source": [
    "## Function to Calculate Precision and Recall\n",
    "In addition to accuracy, we calculate precision = $\\frac{\\text{true positives}}{\\text{false positives + true positives}}$, as well as recall = $\\frac{\\text{true positives}}{\\text{false negatives + true positives}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58012b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_and_recall(matrix):\n",
    "    precision = float(matrix[1][1] / (matrix[1][1] + matrix[1][0]))\n",
    "    recall = float(matrix[1][1] / (matrix[1][1] + matrix[0][1]))\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb11c239",
   "metadata": {},
   "source": [
    "## Run all code!\n",
    "Runs all of the code for Transfer Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f118429f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading in data...\n",
      "Done loading in data.\n",
      "Generate Confusion Matrix\n",
      "[[280, 11], [8, 277]]\n",
      "Precision: 0.9719298245614035\n",
      "Recall: 0.9618055555555556\n"
     ]
    }
   ],
   "source": [
    "# Get and print confusion matrix\n",
    "matrix, real_but_fake, fake_and_fake, all_images = main(model_type = \"TransferLearning\", weights = 'weights/TransferLearning/dalle/TransferLearning-0.6.pth', first='dalle', second='real')\n",
    "print(matrix)\n",
    "\n",
    "# Get and print precision and recall\n",
    "precision, recall = precision_and_recall(matrix)\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77246693",
   "metadata": {},
   "source": [
    "## Function for Total Number of Nouns + Length\n",
    "Using NLTK to calculate total number of nouns and length (number of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee8c8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_and_tokens(description):\n",
    "    tokens = nltk.word_tokenize(description)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    counts = Counter(tag for word,tag in tagged)\n",
    "    num_nouns = counts['NN'] + counts['NNS'] + counts['NNP'] + counts['NNPS']\n",
    "    return num_nouns, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcb0429",
   "metadata": {},
   "source": [
    "## Function for Linguistic Analysis\n",
    "Using the above function by looking at a specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebdcdd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linguistic_analysis(all_images):\n",
    "    # Used to refer to mappings\n",
    "    df = pd.read_csv('dataset/reference.csv')\n",
    "    \n",
    "    # Keeping track of total nouns and lengths\n",
    "    total = 0\n",
    "    all_nouns = []\n",
    "    all_lengths = []\n",
    "    \n",
    "    # Iterate through each imags and calculate\n",
    "    for img in all_images:\n",
    "        # Grab the description\n",
    "        index = int(img[-9:-4])\n",
    "        description = df.iloc[index]['description']\n",
    "        \n",
    "        # Update nouns and tokens variables\n",
    "        num_nouns, tokens = nouns_and_tokens(description)\n",
    "        total += num_nouns\n",
    "        all_nouns.append(num_nouns)\n",
    "        all_lengths.append(len(tokens))\n",
    "    \n",
    "    # Print out statistics on number of nouns\n",
    "    print(\"Number of Nouns:\")\n",
    "    print(\"Mean: \" + str(np.mean(all_nouns)))\n",
    "    print(\"Variance: \" + str(np.var(all_nouns)))\n",
    "    print(\"\")\n",
    "    \n",
    "    # Print out statistics on lengths\n",
    "    print(\"Length of Message:\")\n",
    "    print(\"Mean: \" + str(np.mean(all_lengths)))\n",
    "    print(\"Variance: \" + str(np.var(all_lengths)))\n",
    "    print(\"\")\n",
    "    \n",
    "    return all_lengths, all_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0154a5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Function to Print Specific Nouns\n",
    "We can use this information to suggest an adversarial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1867a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specific_nouns(all_images):\n",
    "    # Used to refer to mappings\n",
    "    df = pd.read_csv('dataset/reference.csv')\n",
    "    \n",
    "    # Keeping track of all nouns and the tags for nouns\n",
    "    all_nouns, nouns_map = [], defaultdict(int)\n",
    "    noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    \n",
    "    # Iterate through each image\n",
    "    for img in all_images:\n",
    "        # Get the description\n",
    "        index = int(img[-9:-4])\n",
    "        description = df.iloc[index]['description']\n",
    "        #print(description)\n",
    "        \n",
    "        # Get the tokens and their tags\n",
    "        tokens = nltk.word_tokenize(description)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        \n",
    "        # Add to set if a noun\n",
    "        for word, tag in tagged:\n",
    "            if (tag in noun_tags):\n",
    "                all_nouns.append(word)\n",
    "                nouns_map[word] += 1\n",
    "                \n",
    "    return all_nouns, dict(sorted(nouns_map.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3163ea7",
   "metadata": {},
   "source": [
    "## Applying Linguistic Analysis Functions\n",
    "Calling above functions to print out data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56aec3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing All Images:\n",
      "Number of Nouns:\n",
      "Mean: 5.826388888888889\n",
      "Variance: 4.553192515432098\n",
      "\n",
      "Length of Message:\n",
      "Mean: 19.416666666666668\n",
      "Variance: 39.31944444444444\n",
      "\n",
      "\n",
      "Analyzing Real, but Fake Images:\n",
      "Number of Nouns:\n",
      "Mean: 5.0\n",
      "Variance: 3.25\n",
      "\n",
      "Length of Message:\n",
      "Mean: 17.625\n",
      "Variance: 11.234375\n",
      "\n",
      "\n",
      "Analyzing Fake Images Classified Correctly:\n",
      "Number of Nouns:\n",
      "Mean: 5.85\n",
      "Variance: 4.5703571428571435\n",
      "\n",
      "Length of Message:\n",
      "Mean: 19.46785714285714\n",
      "Variance: 40.02753826530613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Analyzing All Images:\")\n",
    "all_lengths, all_nouns = linguistic_analysis(all_images)\n",
    "\n",
    "print(\"\\nAnalyzing Real, but Fake Images:\")\n",
    "rbf_lengths, rbf_nouns = linguistic_analysis(real_but_fake)\n",
    "\n",
    "print(\"\\nAnalyzing Fake Images Classified Correctly:\")\n",
    "faf_lengths, faf_nouns = linguistic_analysis(fake_and_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87bd7db",
   "metadata": {},
   "source": [
    "## Super Awesome Bootstrapping Techniques!\n",
    "Ensuring statistical significance to make claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bda371ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapping(total_observations, subsection_of_interest):\n",
    "    sample_mean = np.mean(total_observations)\n",
    "    mean_difference = abs(sample_mean - np.mean(subsection_of_interest))\n",
    "    subsection_length = len(subsection_of_interest)\n",
    "    count = 0.0\n",
    "    iteration_count = 10000\n",
    "    for _ in range(iteration_count):\n",
    "        sampled_lengths = np.random.choice(total_observations, subsection_length, replace=True)\n",
    "        if abs(np.mean(sampled_lengths) - sample_mean) >= mean_difference:\n",
    "            count += 1\n",
    "    print(count / iteration_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb1e362",
   "metadata": {},
   "source": [
    "## Calculating Statistical Significance\n",
    "Calling above function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5b6b212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simple bootstrapping to test against null hypothesis\n",
      "Statistical significance of prompt lengths\n",
      "0.4298\n",
      "Statistical significance of noun counts\n",
      "0.2728\n"
     ]
    }
   ],
   "source": [
    "print(\"Running simple bootstrapping to test against null hypothesis\")\n",
    "print(\"Statistical significance of prompt lengths\")\n",
    "bootstrapping(all_lengths, rbf_lengths)\n",
    "print(\"Statistical significance of noun counts\")\n",
    "bootstrapping(all_nouns, rbf_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c3a68",
   "metadata": {},
   "source": [
    "## Looking at Nouns in Real, but Fake\n",
    "See motivation above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3b3435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at nouns of real, but fake images set:\n",
      "{'ocean': 1, 'Groom': 1, 'tux': 1, 'bride': 1, 'dress': 1, 'flowers': 1, 'grass': 1, 'stick': 1, 'mouth': 1, 'woman': 1, 'rope': 1, 'bridge': 1, 'trees': 1, 'people': 1, 'end': 1, 'adults': 1, 'sit': 1, 'homey': 1, 'porch': 1, 'front': 1, 'flora': 1, 'nature': 1, 'scene': 1, 'toddler': 1, 'grin': 1, 'female': 1, 'boy': 1, 't-ball': 1, 'day': 1, 'dad': 1, 'watching': 1, 'fence': 1, 'men': 1, 'apartment': 1, 'board': 1, 'game': 1, 'man': 1, 'watches': 1, 'dog': 2}\n",
      "Looking at nouns of correctly classified fake images:\n",
      "{'figures': 1, 'ages': 1, 'wooden': 1, 'cross': 1, 'presses': 1, 'desserts': 1, 'tomatoes': 1, 'market': 1, 'buildings': 1, 'shops': 1, 'washers': 1, 'ladders': 1, 'rings': 1, 'spectators': 1, 'toys': 1, 'Woman': 1, 'pad': 1, 'dirty': 1, 'works': 1, 'remodeling': 1, 'project': 1, 'spotlight': 1, 'semi-circle': 1, 'conductor': 1, 'shadow': 1, 'drum': 1, 'overalls': 1, 'stuffed': 1, 'animal': 1, 'drinks': 1, 'horses': 1, 'sunset': 1, 'bricks': 1, 'students': 1, 'bus': 1, 'stop': 1, 'zone': 1, 'Children': 1, 'swing': 1, 'amusement': 1, 'state': 1, 'fair': 1, 'tracks': 1, 'gloves': 1, 'cart': 1, 'fisherman': 1, 'nets': 1, 'haul': 1, 'screaming': 1, 'apron': 1, 'microwave': 1, 'chalkboard': 1, 'dozen': 1, 'pose': 1, 'netting': 1, 'saber': 1, 'laughing': 1, 'heels': 1, 'jungle': 1, 'environment': 1, 'Boys': 1, 'sweatshirts': 1, 'checker': 1, 'waffles': 1, 'hymn': 1, 'congregation': 1, 'tile': 1, 'bathroom': 1, 'slopes': 1, 'pillars': 1, 'lanterns': 1, 'spiderman': 1, 'clime': 1, 'vocalist': 1, 'microphone': 1, 'drummer': 1, 'seat': 1, 'cookie': 1, 'step': 1, 'author': 1, 'Coelho': 1, 'towards': 1, 'arc': 1, 'farmer': 1, 'tractor': 1, 'harvesting': 1, 'crops': 1, 'concrete': 1, 'carriage': 1, 'teen': 1, 'braids': 1, 'pillow': 1, 'adolescent': 1, 'teenager': 1, 'trunks': 1, 'reservoir': 1, 'cement': 1, 'walls': 1, 'appear': 1, 'presentation': 1, 'projector': 1, 'office': 1, 'wand': 1, 'solution': 1, 'laptop': 1, 'microscope': 1, 'circuit': 1, 'device': 1, 'ledge': 1, 'rainbow': 1, 'flags': 1, 'awe': 1, 'mantis': 1, 'church': 1, 'books': 1, 'instructor': 1, 'conducting': 1, 'graffiti': 1, 'chips': 1, 'menu': 1, 'contraption': 1, 'sky': 1, 'White': 1, 'toy': 1, 'towel': 1, 'seeds': 1, 'cooking': 1, 'event': 1, 'eggs': 1, 'ingredients': 1, 'banners': 1, 'officer': 1, 'UH': 1, 'football': 1, 'player': 1, 'teammate': 1, 'blocks': 1, 'opponent': 1, 'French': 1, 'youth': 1, 'protest': 1, 'chant': 1, 'Sarkozy': 1, 'gold': 1, 'backdrop': 1, 'piece': 1, 'waiting': 1, 'speak': 1, 'setter': 1, 'Rottwieler': 1, 'Dalmation': 1, 'hill': 1, 'music': 1, 'guitars': 1, 'violins': 1, 'cellos': 1, 'business': 1, 'lab': 1, 'frolicks': 1, 'knee': 1, 'length': 1, 'vines': 1, 'windows': 1, 'tower': 1, 'barrior': 1, 'Brown': 1, 'puppy': 1, 'toes': 1, 'shovels': 1, 'window': 1, 'neon': 1, 'takeout': 1, 'gymnast': 1, 'balance': 1, 'beam': 1, 'sit': 1, 'lunch': 1, 'meal': 1, 'surf': 1, 'spots': 1, 'karate': 1, 'purple': 1, 'P.I.N.K': 1, 'cheerleader': 1, 'backs': 1, 'elephant': 1, 'shack': 1, 'cockpit': 1, 'airplane': 1, 'aviation': 1, 'museum': 1, 'carnival': 1, 'Green': 1, 'Jungle': 1, 'gentleman': 1, 'bowl': 1, 'goal': 1, 'phone': 1, 'clouds': 1, 'empty': 1, 'mugs': 1, 'cake': 1, 'sprinkler': 1, 'cow': 1, 'behind': 1, 'feet': 1, 'horn': 1, 'Calvin': 1, 'Klein': 1, 'Steel': 1, 'scruffy': 1, 'bike': 1, 'affection': 1, 'escalator': 1, 'Young': 1, 'dock': 1, 'duck': 1, 'swimming': 1, 'jump': 1, 'Nerf': 1, 'swords': 1, 'cup': 1, 'zooms': 1, 'past': 1, 'motor': 1, 'container': 1, 'Blond': 1, 'plants': 1, 'races': 1, 'amount': 1, 'ice': 1, 'fishing': 1, 'mound': 1, 'sand': 1, 'drawn': 1, 'cherry-picker': 1, 'harness': 1, 'smiling': 1, 'Husky': 1, 'straps': 1, 'skateboarding': 1, 'looks': 1, 'metro': 1, 'Climbers': 1, 'direction': 1, 'hamburgers': 1, 'hotdogs': 1, 'stainless': 1, 'steel': 1, 'grill': 1, 'amidst': 1, 'compound': 1, 'Middle': 1, 'Eastern': 1, 'country': 1, 'butt': 1, 'mountains': 1, 'blankets': 1, 'box': 1, 'pier': 1, 'stormy': 1, 'decorations': 1, 'tent': 1, 'float': 1, 'mouths': 1, 'pajamas': 1, 'runs': 1, 'hall': 1, 'hardwood': 1, 'floors': 1, 'motorcycle': 1, 'makeup': 1, 'houses': 1, 'pit': 1, 'fireman': 1, 'jack': 1, 'shopping': 1, 'bags': 1, 'Betty': 1, 'Boop': 1, 'puppies': 1, 'jewelery': 1, 'foreground': 1, 'cyclists': 1, 'others': 1, 'palm': 1, 'terrier': 1, 'points': 1, 'print': 1, 'piggyback': 1, 'apartment': 1, 'complex': 1, 'racquets': 1, 'sight': 1, 'bat': 1, 'waits': 1, 'cone': 1, 'ends': 1, 'shoulder': 1, 'item': 1, 'Person': 1, 'pipe': 1, 'grills': 1, 'climber': 1, 'denim': 1, 'Workers': 1, 'hole': 1, 'tools': 1, 'toddler': 1, 'intimate': 1, 'lighting': 1, 'doors': 1, 'groups': 1, 'pads': 1, 'tray': 1, 'variety': 1, 'swim': 1, 'tub': 1, 'metal': 1, 'handrail': 1, 'Wagamama': 1, 'tipsy': 1, 'Mardi': 1, 'Gras': 1, 'necklaces': 1, 'ale': 1, 'heads': 1, 'expressions': 1, 'tanks': 1, 'order': 1, 'reading': 1, 'sandals': 1, 'help': 1, 'darker': 1, 'legs': 1, 'levels': 1, 'attraction': 1, 'arts': 1, 'mat': 1, 'kneels': 1, 'passenger': 1, 'Speedo': 1, 'underwater': 1, 'peppers': 1, 'grocery': 1, 'stand': 1, 'log': 1, 'off-camera': 1, 'burlap': 1, 'sacks': 1, 'potato': 1, 'sack': 1, 'Shirtless': 1, 'stool': 1, 'mountaineer': 1, 'laughs': 1, 'friend': 1, 'winter': 1, 'branches': 1, 'cane': 1, 'hiker': 1, 'rests': 1, 'sands': 1, 'pulley': 1, 'system': 1, 'tennis': 1, 'balls': 1, 'cracked': 1, 'ties': 1, 'sunglasses': 1, 'tasks': 1, 'bamboo': 1, 'sea': 1, 'foot': 1, 'square': 1, 'type': 1, 'reaches': 1, 'glove': 1, 'pink-and-black': 1, 'socks': 1, 'juggles': 1, 'knives': 1, 'brick': 1, 'Pump': 1, 'Room': 1, 'railway': 1, 'Pedestrians': 1, 'way': 1, 'Times': 1, 'Square': 1, 'New': 1, 'York': 1, 'peace': 1, 'bald': 1, 'spot': 1, 'backseat': 1, 'machine': 1, 'video': 1, 'recording': 1, 'engine': 1, 'playground': 1, 'tube': 1, 'red-hair': 1, 'scaffolding': 1, 'hammer': 1, 'bib': 1, 'crab': 1, 'crustacean': 1, 'stripes': 1, 'flower': 1, 'bank': 1, 'subway': 1, 'cartwheel': 1, 'skin': 1, 'exercise': 1, 'stir': 1, 'fries': 1, 'vegetables': 1, 'home': 1, 'counter': 1, 'jars': 1, 'light': 1, 'conversing': 1, 'passes': 1, 'work': 1, 'windbreakers': 1, 'vests': 1, 'police': 1, 'motorcycles': 1, 'parking': 1, 'Construction': 1, 'job': 1, 'house': 1, 'photographer': 1, 'profile': 1, 'punk': 1, 'hairstyle': 1, 'nose': 1, 'bar': 1, 'hikers': 1, 'summer': 1, 'path': 1, 'meadow': 1, 'day': 1, 'jomps': 1, 'army': 1, 'suits': 1, 'banner': 1, 'Cheer': 1, 'pot': 1, 'household': 1, 'crosswalk': 1, 'rat': 1, 'cut': 1, 'swimsuit': 1, 'letters': 1, 'states': 1, 'Secure': 1, 'Borders': 1, 'Legal': 1, 'Immigration': 1, 'mule': 1, 'backpacks': 1, 'approaches': 1, 'barrier': 1, 'uniform': 1, 'delivery': 1, 'backstroke': 1, 'latte': 1, 'leash': 1, 'uniforms': 1, 'structure': 1, 'Girl': 1, 'multicolor': 1, 'parade': 1, 'prop': 1, 'wheelbarrow': 1, 'number': 1, 'O': 1, 'bride': 1, 'groom': 1, 'joy': 1, 'Great': 1, 'Wall': 1, 'China': 1, 'prayer': 1, 'book': 1, 'Women': 1, 'picnic': 1, 'eating': 1, 'balloon': 1, 'blond-hair': 1, 'wearing': 1, 'midair': 1, 'skateboard': 1, 'left': 1, 'talent': 1, 'watching': 2, 'suit': 2, 'gray': 2, 'paper': 2, 'sweater': 2, 'right': 2, 'coat': 2, 'flag': 2, 'soccer': 2, 'pile': 2, 'school': 2, 'railroad': 2, 'breads': 2, 'pictures': 2, 'silver': 2, 'scene': 2, 'female': 2, 'bikes': 2, 'plastic': 2, 'bag': 2, 'members': 2, 'dragon': 2, 'costume': 2, 'gazebo': 2, 'roof': 2, 'corner': 2, 'stage': 2, 'Light': 2, 'balloons': 2, 'grassy': 2, 'clearing': 2, 'adult': 2, 'board': 2, 'poles': 2, 'arm': 2, 'door': 2, 'leaps': 2, 'Frisbee': 2, 'mother': 2, 'ears': 2, 'stream': 2, 'bird': 2, 'kids': 2, 'band': 2, 'van': 2, 'design': 2, 'instrument': 2, 'backpack': 2, 'trail': 2, 'view': 2, 'risers': 2, 'space': 2, 'balcony': 2, 'wood': 2, 'attire': 2, 'smiles': 2, 'construction': 2, 'camouflage': 2, 'hood': 2, 'barbecue': 2, 'shoes': 2, 'ocean': 2, 'gym': 2, 'mats': 2, 'clothes': 2, 'moves': 2, 'guitar': 2, 'steps': 2, 'kid': 2, 'hockey': 2, 'jersey': 2, 'brother': 2, 'eyes': 2, 'forest': 2, 'stairs': 2, 'sort': 2, 'lone': 2, 'drinking': 2, 'cap': 2, 'heart': 2, 'object': 2, 'sweatshirt': 2, 'station': 2, 'weather': 2, 'shore': 2, 'bucket': 2, 'rope': 2, 'store': 2, 'stone': 2, 'photo': 2, 'guns': 2, 'baseball': 2, 'equipment': 2, 'tables': 2, 'goggles': 2, 'cellphone': 2, 'belts': 2, 'blond': 2, 'race': 2, 'bicycle': 2, 'desert': 2, 'lot': 2, 'bottles': 2, 'shirts': 2, 'living': 2, 'skirt': 2, 'line': 2, 'worker': 2, 'vehicle': 2, 'middle': 2, 'cowboy': 2, 'dirt': 2, 'skateboarder': 2, 'trash': 2, 'standing': 2, 'ladder': 2, 'rocks': 3, 'glass': 3, 'jacket': 3, 'floor': 3, 'tree': 3, 'brown': 3, 'onlookers': 3, 'ride': 3, 'stick': 3, 'guy': 3, 'arms': 3, 'restaurant': 3, 'Man': 3, 'cars': 3, 'watches': 3, 'surface': 3, 'walks': 3, 'baby': 3, 'lap': 3, 'ear': 3, 'mouth': 3, 'someone': 3, 'rock': 3, 'outfit': 3, 'jumps': 3, 'road': 3, 'truck': 3, 'shirtless': 3, 'back': 3, 'pole': 3, 'helmets': 3, 'trick': 3, 'sun': 3, 'helmet': 3, 'stands': 3, 'room': 3, 'dark': 3, 'look': 4, 'city': 4, 'crowd': 4, 'boys': 4, 'park': 4, 'glasses': 4, 'something': 4, 'items': 4, 'food': 4, 'Black': 4, 'jumping': 4, 'face': 4, 'mountain': 4, 'kitchen': 4, 'camera': 4, 'snow': 4, 'car': 4, 'sits': 4, 'fence': 4, 'People': 4, 'body': 4, 'hands': 4, 'side': 4, 'workers': 4, 'boots': 4, 'hats': 4, 'girls': 4, 'beer': 4, 'couple': 4, 'trees': 4, 'flowers': 4, 'choir': 5, 'boat': 5, 'bottle': 5, 'pink': 5, 'wall': 5, 'clothing': 5, 'lake': 5, 'top': 5, 'pool': 5, 'edge': 5, 'safety': 5, 'building': 5, 'track': 5, 'tank': 5, 'ground': 6, 'hat': 6, 'vest': 6, 'head': 6, 'chair': 6, 'picture': 6, 'hand': 6, 'bench': 6, 'sign': 6, 'area': 6, 'person': 6, 'train': 6, 'air': 7, 'blue': 7, 'ball': 7, 'table': 7, 'dress': 7, 'grass': 8, 'sidewalk': 8, 'women': 8, 'pants': 9, 'hair': 9, 'dogs': 9, 'children': 9, 't-shirt': 9, 'jeans': 10, 'beach': 10, 'child': 11, 'street': 12, 'field': 12, 'orange': 12, 'shorts': 13, 'group': 14, 'boy': 15, 'water': 15, 'girl': 16, 'background': 17, 'front': 19, 'men': 20, 'people': 30, 'dog': 34, 'shirt': 42, 'woman': 42, 'man': 84}\n"
     ]
    }
   ],
   "source": [
    "print(\"Looking at nouns of real, but fake images set:\")\n",
    "all_nouns, nouns_map = specific_nouns(real_but_fake)\n",
    "print(nouns_map)\n",
    "\n",
    "print(\"Looking at nouns of correctly classified fake images:\")\n",
    "all_nouns, nouns_map = specific_nouns(fake_and_fake)\n",
    "print(nouns_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e1f4fd-c290-4d7a-bf8c-6403c27c6a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
